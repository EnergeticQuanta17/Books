Vector --> cartesian product of "real" n times

Less conventional notationb --> addition of matrix and vector
	C(i,j) := A(i,j) + B(j)

	vector "B" is added to each row of matrix "A"

Broadcasting --> implicit copying of "B" to many locations

WHAT IF --> matrix multiplication was commutative? --> a shut door in quantum computing

When A^(-1) exists, several different algorithms can find it in closed-form.
	
Usually the precision offered by A^(-1) is very low to solve linear equations. Hence it should not actually be used in practice for most software applications.

Ax = b
	If we can find two solutions to the above equation then the linear combination of the two solutions is also a solution

	Linear combination --> alpha * x + (1-alpha) * y

	Think of columns of A as specifying different directions we can travel in, from origin 
	Then determine how many ways there are of reaching b
	In this view, each element of x tells us how much we must move in that direction

Span --> of a set of vectors --> is the set of all points obtainable by linear combination of the original vectors.

Ax=b is just checking if b is in the span of columns of A. This particular span is known as the "column space" pr the "range" of A.

If A is 3 by 2 matrix, target is 3D and x is 2D, so modifying the value of x at best enables us to trace out a 2D plane within R^3 at best.

Dimensionality condition, {to have infinite solutions} n>=m, is not a sufficient condition because it is possible for some of the columns to be redundant.
	Two columns same

	This kind of redundancy is called "Linear Dependence"

For matrix to have an inverse, we additionally need to ensure that the equation has atmost one solution for each value of b. To do so, we need to make certain that the matrix has atmost m columns
	Together this means that the matrix must be square and that all the columns be linearly independent

If A is not square or is square but singular, solving the equation is still possible but we cannot use the method of matrix inversion to find the solution.

For square matrices, the left inverse and right inverse are equal.

NORMS
	L^p norm -->sum of each element to p-th power then p-th root.

	Norm is any function that satisfies three conditions:-
		1. f(x)=0 implies x=0
		2. f(x+y) <= f(x) + f(y) {traiangle inequality}
		3. for all alpha belonging to Real, f(alpha * x) = mod(alpha) * f(x)

	L^2 for p=2 norm is known as Euclidean norm. --> gives the Euclidean distance from origin

	It is common to measure the size of a vector using this norm.

	Squared L^2 norm --> (x transpose) * (x)

To discriminate between elements that are exactly zero and elements that are small but nonzero --> we use L^1 norm.
	L^1 norm --> is used in ML when the difference between zero and nonzero elements is very important.

We sometimes measure the size of a vector by counting its number of nonzero elements. Some authors refers to this function as L^0 norm, but this is incorrect terminology

Max norm --> L^(inf) norm --> this norm simplifies to the absolute value of the element with largest magnitude in the vector.

Sometimes we wish to measure the size of a matrix.
The most common way to do this is by using --> "Frobenius norm"
	rms of each element of the matrix

Multiplying by a diagonal matrix is computationally efficient
Inverting is also efficient --> it is just reciprocal of individual elements

We usually obtain less expensive algorithm by restricting some matrices to be diagonal.

Nonsquare diagonal matrices do not have inverses, but we can multiply by them cheaply.

Symmetric matrices arise when the entries are generated by some function of two arguments that does not depend on the order of arguments.

FOR VECTORS
	Orthonormal --> orthogonal + unit norm

FOR MATRIX
	Orthogonal matrix --> square matrix whose rows are mututally orthonormal and columns are also mututally orthonormal.
		(A transpose) * (A) = (A) * (A transpose) = I
		A ^ (-1) = (A transpose)

EIGENDECOMPOSITION	
	We can decompose matrices in ways that show us information about their functional properties that is not obvious from the usual representation.

	One of the most widely used kinds of matrix decomposition is called "eigen decomposition"
		--> we decompose into set of eigenvectors and eigenvalues

	An eigenvector of a square matrix is a nonzero vector v such that multiplication by A, alters only the scale of v.

	Suppose that A has n linearly independent eigenvectors with corresponding eigenvalues.
		We may concatenate all the eigenvectors to form a matrix and name it V.
		concatenate all the eigenvalues to form a vector {lambda}

		then, eigen decomposition of A:-
			A = (V) diag({lambda}) (V inverse)

Constructing matrices with certain eigenvalues and eigenvectors enables us to stretch space in desired directions.

Doing decomposition can help us analyze certain properties of the matrix.

Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases complex numbers pop-in.

Every real Symmetric matrix can be decomposed into real valued eigenvectors and eigenvalues
	A = (Q) (lambda) (Q transpose)
		Q is orthogonal

real Symmetric matrix --> may not have unique eigen decomposition
	If two or more eigenvectors share the same eigenvalue, then any set of orthogonal vectors lying in their span are also eigenvectors with that eigenvalue, we can choose a Q using those eigenvectors instead.
		By convention, we usually sort the entries of {lambda} in descending order.
		Under this convention, the eigend decomposition is unique only if all the eigenvalues are unique.

Facts that the eigen decomposition tells us:-
	Matrix is singular iff any of the eigenvalues are zero.

	eigen decomposition of a real symmetric matrix can also be used to optimize quadratic expressions of the form  f(x) = (x transpose) (A) (x)

All eigenvalues are positive --> positive definite
positive + zero --> positive semidefinite
negative + zero --> negative semidefinite

positive semidefinite are interesting because they garuntee that for all x, f(x) is greater than or equal to 0
positive definite matrices additionally garuntee that f(x)=0 imples x=0


SINGULAR VALUE DECOMPOSITION
	Factorize matrix into --> singular vectors + singular values
	SVD is more generally applicable than ED

	Every real matrix has a SVD

	A = (U) (D) (V transpose)

	Three matrices
	A => m x n
	U => m x m
	D => m x n
	V => n x n

	U and V are  defined to be orthogonal matrices.
	D is diagonal matrix

	Elements of D are known as "singular values"
	Columns of U are known as "left-singular values"
	Columns of V are known as "right-singular values"

	We can interpret the SVD of A in terms of ED of functions of A
		left-singular vectors of A are eigenvectors of (A) (A transpose)
		right-singular vectors of A are eigenvectors of (A transpose) (A)

		The nonzero singular values of A are the square roots of the eigenvalues of (A transpose) (A).
		The same is true for (A) (A transpose)

	Most useful feature of SVD --> we can use it to partially generalize matrix inversion to nonsquare matrices


TRACE OPERATOR
	Opens up opportunities to manipulate the expression using many useful identities

	Invariant under transpose

DETERMINANT
	Product of all the eigenvalues of the matrix

	abs of det. --> measure of how much multiplication by the matrix expands or contracts space.

	If det.=0 --> completely contracted along atleast one dimension, causing it to lose all its volume {why does it have to lose volume}

PRINCIPAL COMPONENT ANALYSIS
	Lossy compression --> less memory but may lose some precision.
		We want to lose as less precision as possible

	PCA is defined by our choice of the decoding function.
	To make the decoder simple, we choose the matrix multiplication to map the code back into {R}^n

	Decoder as written in my notes is of dimension --> 'n' x 'l'
	Computing the optimal code for this decoder could be a difficult problem.

	To keep the encoding easy, the PCA constrains the columns of D to be orthogonal to each other {D is still not technically "an orthogonal matrix" --> unless l=n}

	Many solutions to this are possible, because we can increase the scale of D(:,i) if we decrease c(i) proportionally for all points.
		To give the problem a unique solution, we constrain all the columns of D to have unit norm.

	WHAT DOES THIS STATEMENT MEAN --> since we will use the same matrix D to decode all the points, we can no longer consider the points in isolation.
		Instead we must minimize the Frobenius norm of the matrix of errors computed over all dimensions and all points

	Final optimization problem can be solved using eigen decomposition. Specifically, the optimal d is given by the eigenvector of (X transpose) * (X) corresponding to the largest eigenvalue.


	This derivation is specific to l=1 and recovers only the first principal component.
	If we wish to recover a basis of principal components, the matrix D is given by the l eigenvectors corresponding to the largest eigenvalues. This may be shown using proof by induction.
	